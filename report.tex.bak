\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}

\title{Homework 5: Reinforcement Learning}
\author{Shun Zhang}
\date{}

\begin{document}
\maketitle

\section{Q-Learning}

Q-Learning is an off-policy temporal difference learning algorithm. It
keeps a Q-table which evaluates each state, action pair. The idea is
to update Q-values with the observed sample, with the learning rate of
$\alpha$, shown in the equation below.

$$Q(s, a) = (1 - \alpha) Q(s, a) + \alpha (r + \gamma Q(s', a'))$$

In this assignment, I need to extract features in the domains. For
example, I represent the states by the distance to the nearest
obstacle in the obstacle domain (will be described in the experiment
section). Let $f_i$ be the i-th feature, and $w_i$ be the weight of
the i-th feature, the Q function can be represented as

$$Q(s, a) = \sum w_i f_i(s, a)$$

To update $Q(s, a)$, $w_i$ needs to be updated. As $\frac{\partial
Q(s, a)}{\partial w_i} = f_i(s, a)$, the update mechanism can be
changed to be as follows.

$$correction = r + \gamma Q(s', a') - Q(s, a)$$
$$w_i = w_i + \alpha [correction] f_i(s, a)$$

\section{Modular RL}

The task is to combine some sub-MDPs, each of which has some perticular
behavior, to a hybrid MDP.

\subsection{Linear Combination}

One easy way to mix sub-MPDs is representing the Q-value of the hybrid
MDP as linear combination as sub-MDPs. That is,
$$Q(s, a) = \sum{\beta_i Q_i(s, a)}$$
where $\beta_i$ is the weight on the i-th sub-MDP. $Q_i$ is the
Q-value of the i-th sub-MDP.

We need to hand-tune the parameter $\beta$ to make $Q$ weight the
sub-MDPs reasonably.

\subsection{Gibbs Softmax}
\label{sec:gibbs}

The motivation of using Gibbs softmax function is to normalize the Q-values over the actions given a state.

There could be a bias in the Q-values in one sub-MDP. When such
bias increase, the Q-values increase so that such sub-MDP will
unnecessarily dominates. So I normalize the Q-values by Gibbs softmax
function, defined as below.

$$Q'_i(s, a) = \frac{e^{Q_i(s, a)}}{\sum\limits_{a \in A(s)}e^{Q_i(s, a)}}$$
$$Q(s, a) = \sum{\beta_i Q'_i(s, a)}$$
where $A(s)$ is the set of available actions in the state $s$.
$Q'_i$ is the normalized Q-value.

\section{Experiments}

Modular RL is evaluated in a grid world domain, in which the agent
needs to reach the targets while avoiding the obstacles. First, I
train two MDPs for walking towards the goal and obstacle avoiding,
respectively. Then, I combine these two MDPs for the desired
performance.

\subsection{Approaching-the-Goal Module}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.7\textwidth}
	\includegraphics[width=\textwidth]{figure/sidewalk}
\end{subfigure}
\begin{subfigure}{0.7\textwidth}
	\includegraphics[width=\textwidth]{figure/sidewalk_v}
\end{subfigure}
\caption{Approaching-the-goal module.}
\label{fig:sidewalk}
\end{figure}

Figure~\ref{fig:sidewalk} shows the learned Q-values (the upper graph),
values and policy (the lower graph) for approaching the goal. There is
reward of +1 of reaching the goals, which are the right-most column.
$\epsilon = 0.3, \gamma = 0.5$. I keep an exploration rate larger than
0.1, so that it can explore the whole space. The discount factor
$\gamma$ is small here, so the agent will aim at the shortest path,
instead of wasting time in the grids. I used one feature $x$ to
represent each state, which is the horizontal coordinate of the
position of the agent. The learned parameter is

\texttt{{'x': 0.06250000371801567}}

\subsection{Obstacle-Avoiding Module}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\textwidth]{figure/obstacle}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\textwidth]{figure/obstacle_policy}
\end{subfigure}
\caption{Obstacle-avoiding module.}
\label{fig:obstacle}
\end{figure}

In Figure~\ref{fig:obstacle}, similarly, I trained an
obstacle-avoiding agent. There is a -1 reward in the center of the
grid world. $\epsilon = 0.3, \gamma = 0.9$. There are two features,
$bias$, which is always 1, and $dis$ which is the distance to the
obstacle. The learned parameters are

\texttt{{'bias': -0.20931133310480204, 'dis': 0.06742681562641269}}

Note that I only put one obstacle (the grid in the center). As the
agent only consider the closest obstacle, so there should be no
difference between using one and multiple obstacles.

\subsection{Hybrid MDP: Linear Combination}

For the Hybrid domain, I keep the goals at the right-most column. I
randomly set some obstacles in the domain.

\begin{figure}[h!]
\centering
\begin{subfigure}{0.7\textwidth}
	\includegraphics[width=\textwidth]{figure/hybird_walk}
	\caption{0.9 approaching + 0.1 avoiding}
\end{subfigure}
\begin{subfigure}{0.7\textwidth}
	\includegraphics[width=\textwidth]{figure/hybird_obstacle}
	\caption{0.1 approaching + 0.9 avoiding}
\end{subfigure}
\caption{Overweighting one module makes the behavior of that module
dominate.}
\label{fig:lin_test}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\textwidth]{figure/hybird_6_4}
\caption{0.4 approaching + 0.6 avoiding}
\label{fig:lin_opt}
\end{figure}

I tried some linear combination of sub-MDPs. If I overweight one
module, then the behavior of that module would dominate
(Figure~\ref{fig:lin_test}). Setting the weights to be 0.4 for
approaching module and 0.6 for avoiding module achieves the best
solution in my trials (Figure~\ref{fig:lin_opt}). Now I observe the
problem described in the instruction -- the agent will walk over the
obstacle near the goal, because the Q-value of approaching the goal is
much higher. So I normalize them in the next section.

\subsection{Hybird MDP: Gibbs Softmax Function}

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\textwidth]{figure/hybird_gibbs}
\caption{Using Gibbs softmax function.}
\label{fig:gibbs}
\end{figure}

Using the method described in Section~\ref{sec:gibbs}, I find the
results in Figure~\ref{fig:gibbs}. For the grid (3, 0), the Q-values
of going up and down are the same, so the agent should randomly choose
one. Therefore this solution is correct.

\subsection{On Larger Grid}

\begin{figure}[h!]
\centering
\begin{subfigure}{\textwidth}
	\includegraphics[width=\textwidth]{figure/long_1}
\end{subfigure}
\begin{subfigure}{\textwidth}
	\includegraphics[width=\textwidth]{figure/long_2}
\end{subfigure}
\caption{Overweighting one module makes the behavior of that module
dominate.}
\label{fig:long}
\end{figure}

I further tested this Modular MDP using Gibbs Softmax Function in a 3
x 25 grid world (Figure~\ref{fig:long}). It is a long graph so I broke
it into two parts. The policy are mostly correct.

\section{Discussion and Conclusion}

In my view, modular RL is one type of transfer learning. It apply the
results in some predefined source tasks to the target task. Also, some
learning could be allowed after combining the source tasks, rather
than our hand-tuning.

\end{document}
