Path following
- reward by shrinking distance to next waypoint
- switch between waypoints consecutively

contacted number of objects - output to stats

q
- fine-grined q table
- potential based
  - decompose path module to be two seperate ones?

sanity check: weights and discounters - TODO

From Matt, run path module training in this way:
if the agent takes a step that makes progress towards its goal (d_2 < d_1) AND
it is not too far from the actual path (dist_to_path < threshold), it gets
rewarded.  I think the amount of reward should fall off with increasing
dist_to_path - I used 1-dist_to_path^2 as my threshold was 1 - small breaks
from the path should have little penalty while being close to past threshold
should be about as bad as past it. Definitely multiple ways to alter the
reward, but this seemed a decent first approximation. I can think of other
things to try.
- Will this oscillate?

From Ruohan:
consider all the module instances - makes sense
big target at the final dest - ?
