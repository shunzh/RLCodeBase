Path following
transition:
- reward by shrinking distance to next waypoint
- switch between waypoints consecutively
reward:
- potential based - using
- Matt's way - implemented

q
- fine-grained q table - tried it
  - even smaller bins around the center
- potential based

sanity check
- check path constraint module again
- evaluation
  - probability of observing human data given model - done
  - baselines: reflex - done

From Matt
- symmetric q tables (in G; L and R) - overwrote state mapper
- q learning: decrease learning rate
  - trying 0.1 rather than 0.5 first. decreasing learning rate may not reach
    the optimum (while constant learning rate may overshoot).
- bin sizes at center - OK
- path module: more waypoints - OK, using 3
- larger obstacle - See Apr.12 notes
- Mar.10: shape of V
- Apr.12: reward for path - OK
- guassian blur. actually use eligibility traces
- Color human segment.
- 5 actions - OK
  15 deg (2/3), 45 deg (1/3).

- convert to convext opt problem - OK

DOUBTS
- weights and discounters, forward then backward - necessary?
- consider all the module instances - necessary?

==============
TODO

- Evaluation: Bayesian IRL

- May try CMA-ES if DE is too slow.

- multiple waypoints ahead
- angular difference

- visual cone

logis:
- merge separate condor out files. fine
