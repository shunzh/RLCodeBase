Path following
transition:
- reward by shrinking distance to next waypoint
- switch between waypoints consecutively
reward:
- potential based - using
- Matt's way - implemented

q
- fine-grained q table - tried it
  - even smaller bins around the center
- potential based

sanity check
- check path constraint module again
- evaluation
  - probability of observing human data given model - done
  - baselines: reflex - done

From Matt
- symmetric q tables (in G; L and R) - overwrote state mapper
- q learning: decrease learning rate
  - trying 0.1 rather than 0.5 first. decreasing learning rate may not reach
    the optimum (while constant learning rate may overshoot).
- bin sizes at center - OK
- path module: more waypoints - OK, using 3
- larger obstacle - See Apr.12 notes

==============
TODO

- Evaluation: Ng's IRL
- weights and discounters, forward then backward
- consider all the module instances - makes sense, go for it
